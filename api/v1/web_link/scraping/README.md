# Web Scraping com Selenium

Este m√≥dulo implementa web scraping usando **Selenium WebDriver** com Chrome headless, incluindo estrat√©gias anti-detec√ß√£o para contornar bloqueios de sites.

## üöÄ Funcionalidades

- **Selenium WebDriver**: Renderiza JavaScript e conte√∫do din√¢mico
- **Chrome Headless**: Execu√ß√£o sem interface gr√°fica
- **Anti-detec√ß√£o**: 
  - User-Agent aleat√≥rio
  - Delays aleat√≥rios entre a√ß√µes
  - Mascaramento de propriedades do Selenium
  - Scroll suave para simular comportamento humano
- **Espera inteligente**: Aguarda elementos da p√°gina carregarem
- **Retry autom√°tico**: At√© 3 tentativas em caso de falha
- **Extra√ß√£o robusta**: BeautifulSoup para parsing do HTML renderizado

## üì¶ Depend√™ncias

As seguintes depend√™ncias foram adicionadas ao `requirements.txt`:

```
selenium==4.27.1
webdriver-manager==4.0.2
```

## üê≥ Docker

O `Dockerfile.worker` foi atualizado para incluir:

- Google Chrome Stable
- ChromeDriver (gerenciado automaticamente via webdriver-manager)
- Depend√™ncias do sistema necess√°rias

## üîß Uso

```python
from api.v1.web_link.scraping.scraping import url_to_json

# Faz scraping de uma URL
page_content = url_to_json(
    url="https://example.com",
    timeout=30.0,  # Timeout em segundos (padr√£o: 30s)
    max_retries=2  # N√∫mero de tentativas (padr√£o: 2)
)

# Acessa os dados extra√≠dos
print(page_content.title)
print(page_content.description)
print(page_content.text_full)
```

## üõ†Ô∏è Instala√ß√£o Local (Desenvolvimento)

Para testar localmente, voc√™ precisa instalar o Chrome/Chromium:

### Ubuntu/Debian:
```bash
wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
sudo apt-get update
sudo apt-get install google-chrome-stable
```

### macOS:
```bash
brew install --cask google-chrome
```

### Instalar depend√™ncias Python:
```bash
pip install -r requirements.txt
```

## üö¶ Deploy

No deploy com Docker, o Chrome j√° estar√° instalado automaticamente via `Dockerfile.worker`.

Para rebuild do container worker:

```bash
docker-compose build worker
docker-compose up -d worker
```

## ‚öôÔ∏è Configura√ß√µes Anti-Detec√ß√£o

O scraper implementa v√°rias t√©cnicas para evitar detec√ß√£o:

1. **User-Agent Rotation**: 6 diferentes user-agents s√£o alternados aleatoriamente
2. **Mascaramento de WebDriver**: Remove propriedades que identificam Selenium
3. **Delays Aleat√≥rios**: Entre 1-3s para simular leitura humana
4. **Scroll Behavior**: Simula scroll natural da p√°gina
5. **Window Size**: Define tamanho de janela realista (1920x1080)

## üìù Logs

O m√≥dulo gera logs detalhados para debug:

```
[SELENIUM] Tentativa 1/3 - Acessando: https://example.com
[SELENIUM] P√°gina carregada com sucesso: https://example.com
```

## ‚ö†Ô∏è Limita√ß√µes

- **CAPTCHA**: N√£o h√° suporte para resolu√ß√£o autom√°tica de CAPTCHA
- **Recursos**: Consome mais mem√≥ria e CPU que scraping via `requests`
- **Velocidade**: Mais lento que scraping tradicional (necess√°rio para bypass de prote√ß√µes)

## üîç Troubleshooting

### Chrome n√£o encontrado no container:
```bash
# Verificar instala√ß√£o
docker-compose exec worker google-chrome --version

# Rebuild do container
docker-compose build --no-cache worker
```

### Timeout frequente:
```python
# Aumente o timeout
page_content = url_to_json(url, timeout=60.0, max_retries=3)
```

### Erro de WebDriver:
O `webdriver-manager` baixa automaticamente o ChromeDriver compat√≠vel. Se houver erro, pode ser problema de rede ou permiss√µes.

## üìÑ Estrutura de Retorno

```python
PageContent(
    title: str | None,
    description: str | None,
    keywords: str | None,
    canonical: str | None,
    headings: HeadingsData(h1=[], h2=[], h3=[]),
    text_full: str,  # At√© 20KB de texto
    og: OpenGraphData(type=None, url=None, image=None)
)
```

